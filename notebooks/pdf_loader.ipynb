{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c879503",
   "metadata": {},
   "source": [
    "### RAG Pipelines- Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6377b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pal194/miniconda3/envs/py_rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a312dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files to process\n",
      "\n",
      "Processing: Deep learning.pdf\n",
      "  ✓ Loaded 1 pages\n",
      "\n",
      "Total documents loaded: 1\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96748c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'LibreOffice 7.3', 'creator': 'Writer', 'creationdate': '2025-12-01T15:43:44+10:00', 'author': 'User pc', 'source': '../data/pdf/Deep learning.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Deep learning.pdf', 'file_type': 'pdf'}, page_content='Deep  learning is a branch  of machine learning  that uses  multi-layered  artificial  neural  networks  to\\nautomatically learn patterns and representations from data. Instead of relying on hand-crafted features, deep\\nmodels  discover  useful  structures  directly  from  raw  inputs  like  images,  text,  or  audio  by  stacking  many\\ntransformation layers. Early layers often learn simple patterns (edges, shapes, or word co-occurrences), while\\ndeeper layers capture more abstract concepts (objects, semantics, or intent). This hierarchical representation\\nlearning has enabled breakthroughs in computer vision, natural language processing, speech recognition, and\\nmany other fields. However, deep learning also comes with challenges, including large data and computation\\nrequirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\\nongoing research into more efficient, robust, and trustworthy models.\\nDeep learning is a powerful technique where computers learn to make decisions by passing data through\\nmany layers of artificial neurons. Each layer transforms the input a little bit, gradually turning raw data—such\\nas pixels, sound waves, or sensor readings—into meaningful features like shapes, words, or actions. By training\\non large datasets, deep networks can recognize objects in images, translate between languages, drive cars, or\\neven generate realistic text and images. Unlike traditional methods that require manual feature engineering,\\ndeep  learning  automates  this  process,  often  achieving  superior  performance.  At  the  same  time,  it  raises\\nimportant questions about transparency, fairness, and robustness, which are now central topics in modern AI\\nresearch.\\nPage 1 of 1')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab6b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b75a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 2 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Deep  learning is a branch  of machine learning  that uses  multi-layered  artificial  neural  networks  to\n",
      "automatically learn patterns and representations from data. Instead of relying on hand-craft...\n",
      "Metadata: {'producer': 'LibreOffice 7.3', 'creator': 'Writer', 'creationdate': '2025-12-01T15:43:44+10:00', 'author': 'User pc', 'source': '../data/pdf/Deep learning.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Deep learning.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'LibreOffice 7.3', 'creator': 'Writer', 'creationdate': '2025-12-01T15:43:44+10:00', 'author': 'User pc', 'source': '../data/pdf/Deep learning.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Deep learning.pdf', 'file_type': 'pdf'}, page_content='Deep  learning is a branch  of machine learning  that uses  multi-layered  artificial  neural  networks  to\\nautomatically learn patterns and representations from data. Instead of relying on hand-crafted features, deep\\nmodels  discover  useful  structures  directly  from  raw  inputs  like  images,  text,  or  audio  by  stacking  many\\ntransformation layers. Early layers often learn simple patterns (edges, shapes, or word co-occurrences), while\\ndeeper layers capture more abstract concepts (objects, semantics, or intent). This hierarchical representation\\nlearning has enabled breakthroughs in computer vision, natural language processing, speech recognition, and\\nmany other fields. However, deep learning also comes with challenges, including large data and computation\\nrequirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\\nongoing research into more efficient, robust, and trustworthy models.'),\n",
       " Document(metadata={'producer': 'LibreOffice 7.3', 'creator': 'Writer', 'creationdate': '2025-12-01T15:43:44+10:00', 'author': 'User pc', 'source': '../data/pdf/Deep learning.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Deep learning.pdf', 'file_type': 'pdf'}, page_content='requirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\\nongoing research into more efficient, robust, and trustworthy models.\\nDeep learning is a powerful technique where computers learn to make decisions by passing data through\\nmany layers of artificial neurons. Each layer transforms the input a little bit, gradually turning raw data—such\\nas pixels, sound waves, or sensor readings—into meaningful features like shapes, words, or actions. By training\\non large datasets, deep networks can recognize objects in images, translate between languages, drive cars, or\\neven generate realistic text and images. Unlike traditional methods that require manual feature engineering,\\ndeep  learning  automates  this  process,  often  achieving  superior  performance.  At  the  same  time,  it  raises\\nimportant questions about transparency, fairness, and robustness, which are now central topics in modern AI\\nresearch.\\nPage 1 of 1')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe92ea",
   "metadata": {},
   "source": [
    "### embedding And vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3ae3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d26ff3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pal194/miniconda3/envs/py_rag/lib/python3.10/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2 on cpu\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x785efbe72080>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", device: str | None = None):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "            device: \"cpu\" or \"cuda\"; if None, auto-detect\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        # auto-detect, but stay on CPU if CUDA isn't cleanly available\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name} on {self.device}\")\n",
    "            self.model = SentenceTransformer(self.model_name, device=self.device)\n",
    "            print(\n",
    "                f\"Model loaded successfully. \"\n",
    "                f\"Embedding dimension: {self.model.get_sentence_embedding_dimension()}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts on {self.device}...\")\n",
    "        embeddings = self.model.encode(\n",
    "            texts,\n",
    "            show_progress_bar=True,\n",
    "            device=self.device,  # be explicit here too\n",
    "        )\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c9e3b",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c276d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x7860a4493b50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5d2c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'LibreOffice 7.3', 'creator': 'Writer', 'creationdate': '2025-12-01T15:43:44+10:00', 'author': 'User pc', 'source': '../data/pdf/Deep learning.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Deep learning.pdf', 'file_type': 'pdf'}, page_content='Deep  learning is a branch  of machine learning  that uses  multi-layered  artificial  neural  networks  to\\nautomatically learn patterns and representations from data. Instead of relying on hand-crafted features, deep\\nmodels  discover  useful  structures  directly  from  raw  inputs  like  images,  text,  or  audio  by  stacking  many\\ntransformation layers. Early layers often learn simple patterns (edges, shapes, or word co-occurrences), while\\ndeeper layers capture more abstract concepts (objects, semantics, or intent). This hierarchical representation\\nlearning has enabled breakthroughs in computer vision, natural language processing, speech recognition, and\\nmany other fields. However, deep learning also comes with challenges, including large data and computation\\nrequirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\\nongoing research into more efficient, robust, and trustworthy models.'),\n",
       " Document(metadata={'producer': 'LibreOffice 7.3', 'creator': 'Writer', 'creationdate': '2025-12-01T15:43:44+10:00', 'author': 'User pc', 'source': '../data/pdf/Deep learning.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'Deep learning.pdf', 'file_type': 'pdf'}, page_content='requirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\\nongoing research into more efficient, robust, and trustworthy models.\\nDeep learning is a powerful technique where computers learn to make decisions by passing data through\\nmany layers of artificial neurons. Each layer transforms the input a little bit, gradually turning raw data—such\\nas pixels, sound waves, or sensor readings—into meaningful features like shapes, words, or actions. By training\\non large datasets, deep networks can recognize objects in images, translate between languages, drive cars, or\\neven generate realistic text and images. Unlike traditional methods that require manual feature engineering,\\ndeep  learning  automates  this  process,  often  achieving  superior  performance.  At  the  same  time,  it  raises\\nimportant questions about transparency, fairness, and robustness, which are now central topics in modern AI\\nresearch.\\nPage 1 of 1')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bde24ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 2 texts on cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (2, 384)\n",
      "Adding 2 documents to vector store...\n",
      "Successfully added 2 documents to vector store\n",
      "Total documents in collection: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store in the vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498acd10",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7b0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351730b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x785efb46a860>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7e78529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is deep learning?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts on cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_d13a4fcb_1',\n",
       "  'content': 'requirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\\nongoing research into more efficient, robust, and trustworthy models.\\nDeep learning is a powerful technique where computers learn to make decisions by passing data through\\nmany layers of artificial neurons. Each layer transforms the input a little bit, gradually turning raw data—such\\nas pixels, sound waves, or sensor readings—into meaningful features like shapes, words, or actions. By training\\non large datasets, deep networks can recognize objects in images, translate between languages, drive cars, or\\neven generate realistic text and images. Unlike traditional methods that require manual feature engineering,\\ndeep  learning  automates  this  process,  often  achieving  superior  performance.  At  the  same  time,  it  raises\\nimportant questions about transparency, fairness, and robustness, which are now central topics in modern AI\\nresearch.\\nPage 1 of 1',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'author': 'User pc',\n",
       "   'source': '../data/pdf/Deep learning.pdf',\n",
       "   'doc_index': 1,\n",
       "   'total_pages': 1,\n",
       "   'creationdate': '2025-12-01T15:43:44+10:00',\n",
       "   'content_length': 979,\n",
       "   'page_label': '1',\n",
       "   'creator': 'Writer',\n",
       "   'producer': 'LibreOffice 7.3',\n",
       "   'page': 0,\n",
       "   'source_file': 'Deep learning.pdf'},\n",
       "  'similarity_score': 0.37447452545166016,\n",
       "  'distance': 0.6255254745483398,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_66cf337b_1',\n",
       "  'content': 'requirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\\nongoing research into more efficient, robust, and trustworthy models.\\nDeep learning is a powerful technique where computers learn to make decisions by passing data through\\nmany layers of artificial neurons. Each layer transforms the input a little bit, gradually turning raw data—such\\nas pixels, sound waves, or sensor readings—into meaningful features like shapes, words, or actions. By training\\non large datasets, deep networks can recognize objects in images, translate between languages, drive cars, or\\neven generate realistic text and images. Unlike traditional methods that require manual feature engineering,\\ndeep  learning  automates  this  process,  often  achieving  superior  performance.  At  the  same  time,  it  raises\\nimportant questions about transparency, fairness, and robustness, which are now central topics in modern AI\\nresearch.\\nPage 1 of 1',\n",
       "  'metadata': {'author': 'User pc',\n",
       "   'content_length': 979,\n",
       "   'creationdate': '2025-12-01T15:43:44+10:00',\n",
       "   'page': 0,\n",
       "   'producer': 'LibreOffice 7.3',\n",
       "   'doc_index': 1,\n",
       "   'file_type': 'pdf',\n",
       "   'page_label': '1',\n",
       "   'creator': 'Writer',\n",
       "   'source_file': 'Deep learning.pdf',\n",
       "   'total_pages': 1,\n",
       "   'source': '../data/pdf/Deep learning.pdf'},\n",
       "  'similarity_score': 0.37447452545166016,\n",
       "  'distance': 0.6255254745483398,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_79372547_1',\n",
       "  'content': 'requirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\\nongoing research into more efficient, robust, and trustworthy models.\\nDeep learning is a powerful technique where computers learn to make decisions by passing data through\\nmany layers of artificial neurons. Each layer transforms the input a little bit, gradually turning raw data—such\\nas pixels, sound waves, or sensor readings—into meaningful features like shapes, words, or actions. By training\\non large datasets, deep networks can recognize objects in images, translate between languages, drive cars, or\\neven generate realistic text and images. Unlike traditional methods that require manual feature engineering,\\ndeep  learning  automates  this  process,  often  achieving  superior  performance.  At  the  same  time,  it  raises\\nimportant questions about transparency, fairness, and robustness, which are now central topics in modern AI\\nresearch.\\nPage 1 of 1',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'author': 'User pc',\n",
       "   'source_file': 'Deep learning.pdf',\n",
       "   'total_pages': 1,\n",
       "   'creationdate': '2025-12-01T15:43:44+10:00',\n",
       "   'doc_index': 1,\n",
       "   'content_length': 979,\n",
       "   'source': '../data/pdf/Deep learning.pdf',\n",
       "   'page': 0,\n",
       "   'creator': 'Writer',\n",
       "   'page_label': '1',\n",
       "   'producer': 'LibreOffice 7.3'},\n",
       "  'similarity_score': 0.37447452545166016,\n",
       "  'distance': 0.6255254745483398,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_6377eb99_0',\n",
       "  'content': 'Deep  learning is a branch  of machine learning  that uses  multi-layered  artificial  neural  networks  to\\nautomatically learn patterns and representations from data. Instead of relying on hand-crafted features, deep\\nmodels  discover  useful  structures  directly  from  raw  inputs  like  images,  text,  or  audio  by  stacking  many\\ntransformation layers. Early layers often learn simple patterns (edges, shapes, or word co-occurrences), while\\ndeeper layers capture more abstract concepts (objects, semantics, or intent). This hierarchical representation\\nlearning has enabled breakthroughs in computer vision, natural language processing, speech recognition, and\\nmany other fields. However, deep learning also comes with challenges, including large data and computation\\nrequirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\\nongoing research into more efficient, robust, and trustworthy models.',\n",
       "  'metadata': {'author': 'User pc',\n",
       "   'source_file': 'Deep learning.pdf',\n",
       "   'producer': 'LibreOffice 7.3',\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 1,\n",
       "   'source': '../data/pdf/Deep learning.pdf',\n",
       "   'page_label': '1',\n",
       "   'creator': 'Writer',\n",
       "   'content_length': 959,\n",
       "   'doc_index': 0,\n",
       "   'page': 0,\n",
       "   'creationdate': '2025-12-01T15:43:44+10:00'},\n",
       "  'similarity_score': 0.36335331201553345,\n",
       "  'distance': 0.6366466879844666,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_011fdaee_0',\n",
       "  'content': 'Deep  learning is a branch  of machine learning  that uses  multi-layered  artificial  neural  networks  to\\nautomatically learn patterns and representations from data. Instead of relying on hand-crafted features, deep\\nmodels  discover  useful  structures  directly  from  raw  inputs  like  images,  text,  or  audio  by  stacking  many\\ntransformation layers. Early layers often learn simple patterns (edges, shapes, or word co-occurrences), while\\ndeeper layers capture more abstract concepts (objects, semantics, or intent). This hierarchical representation\\nlearning has enabled breakthroughs in computer vision, natural language processing, speech recognition, and\\nmany other fields. However, deep learning also comes with challenges, including large data and computation\\nrequirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\\nongoing research into more efficient, robust, and trustworthy models.',\n",
       "  'metadata': {'source_file': 'Deep learning.pdf',\n",
       "   'producer': 'LibreOffice 7.3',\n",
       "   'total_pages': 1,\n",
       "   'file_type': 'pdf',\n",
       "   'author': 'User pc',\n",
       "   'doc_index': 0,\n",
       "   'content_length': 959,\n",
       "   'source': '../data/pdf/Deep learning.pdf',\n",
       "   'page_label': '1',\n",
       "   'creationdate': '2025-12-01T15:43:44+10:00',\n",
       "   'creator': 'Writer',\n",
       "   'page': 0},\n",
       "  'similarity_score': 0.36335331201553345,\n",
       "  'distance': 0.6366466879844666,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is deep learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23783e",
   "metadata": {},
   "source": [
    "### RAG Pipeline- VectorDB To LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "449a65c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba4b617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40bba05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"llama-3.3-70b-versatile\", api_key: str =None):\n",
    "        \"\"\"\n",
    "        Initialize Groq LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: Groq model name (qwen2-72b-instruct, llama3-70b-8192, etc.)\n",
    "            api_key: Groq API key (or set GROQ_API_KEY environment variable)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        print(self.api_key)\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved document context\n",
    "            max_length: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Generated response string\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Simple response generation without complex prompting\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved context\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1fc0f74",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GroqLLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize Groq LLM (you'll need to set GROQ_API_KEY environment variable)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     groq_llm \u001b[38;5;241m=\u001b[39m \u001b[43mGroqLLM\u001b[49m(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovie_key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroq LLM initialized successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GroqLLM' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize Groq LLM (you'll need to set GROQ_API_KEY environment variable)\n",
    "try:\n",
    "    # groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    groq_llm = GroqLLM(api_key=\"provie_key\")\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4110c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Unified Multi-task Learning Framework'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts on cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the context from the retriever and pass it to the LLM\n",
    "\n",
    "rag_retriever.retrieve(\"Unified Multi-task Learning Framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea465ac",
   "metadata": {},
   "source": [
    "### Integration Vectordb Context pipeline With LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a950a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.3-70b-versatile\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df1bf366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is Deep learning?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts on cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 144.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a powerful technique where computers learn to make decisions by passing data through many layers of artificial neurons, transforming raw data into meaningful features.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is Deep learning?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857b1c2",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2832fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Hard Negative Mining Technqiues'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts on cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "Answer: No relevant context found.\n",
      "Sources: []\n",
      "Confidence: 0.0\n",
      "Context Preview: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"Hard Negative Mining Technqiues\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa6150d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is deep learning'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts on cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 102.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "requirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\n",
      "ongoing research into more efficient, robust, and trustworthy models.\n",
      "Deep learning is a powerful technique where computers learn to ma"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ke decisions by passing data through\n",
      "many layers of artificial neurons. Each layer transforms the input a little bit, gradually turning raw data—such\n",
      "as pixels, sound waves, or sensor readings—into meaningful features like shapes, words, or actions. By training\n",
      "on large datasets, deep networks can recognize objects in images, translate between languages, drive cars, or\n",
      "even generate realistic text and images. Unlike traditional methods that require manual feature engineering,\n",
      "deep  learning  automates  this  process,  often  achieving  superior  performance.  At  the  same  time,  it  raises\n",
      "important questions about transparency, fairness, and robustness, which are now central topics in modern AI\n",
      "research.\n",
      "Page 1 of 1\n",
      "\n",
      "requirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\n",
      "ongoing research into more efficient, robust, and trustworthy models.\n",
      "Deep learning is a powerful technique where computers learn to make decisions by passing data through\n",
      "many layers of artificial neurons. Each layer transforms the input a little bit, gradually turning raw data—such\n",
      "as pixels, sound waves, or sensor readings—into meaningful features like shapes, words, or actions. By training\n",
      "on large datasets, deep networks can recognize objects in images, translate between languages, drive cars, or\n",
      "even generate realistic text and images. Unlike traditional methods that require manual feature engineering,\n",
      "deep  learning  automates  this  process,  often  achieving  superior  performance.  At  the  same  time,  it  raises\n",
      "important questions about transparency, fairness, and robustness, which are now central topics in modern AI\n",
      "research.\n",
      "Page 1 of 1\n",
      "\n",
      "requirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\n",
      "ongoing research into more efficient, robust, and trustworthy models.\n",
      "Deep learning is a powerful technique where computers learn to make decisions by passing data through\n",
      "many layers of artificial neurons. Each layer transforms the input a little bit, gradually turning raw data—such\n",
      "as pixels, sound waves, or sensor readings—into meaningful features like shapes, words, or actions. By training\n",
      "on large datasets, deep networks can recognize objects in images, translate between languages, drive cars, or\n",
      "even generate realistic text and images. Unlike traditional methods that require manual feature engineering,\n",
      "deep  learning  automates  this  process,  often  achieving  superior  performance.  At  the  same  time,  it  raises\n",
      "important questions about transparency, fairness, and robustness, which are now central topics in modern AI\n",
      "research.\n",
      "Page 1 of 1\n",
      "\n",
      "Question: what is deep learning\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: Deep learning is a powerful technique where computers learn to make decisions by passing data through many layers of artificial neurons, transforming raw data into meaningful features.\n",
      "\n",
      "Citations:\n",
      "[1] Deep learning.pdf (page 0)\n",
      "[2] Deep learning.pdf (page 0)\n",
      "[3] Deep learning.pdf (page 0)\n",
      "Summary: Deep learning is a technique that enables computers to make decisions by processing data through multiple layers of artificial neurons. This process transforms raw data into meaningful features, allowing computers to learn and make informed decisions.\n",
      "History: {'question': 'what is deep learning', 'answer': 'Deep learning is a powerful technique where computers learn to make decisions by passing data through many layers of artificial neurons, transforming raw data into meaningful features.', 'sources': [{'source': 'Deep learning.pdf', 'page': 0, 'score': 0.40924692153930664, 'preview': 'requirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\\nongo...'}, {'source': 'Deep learning.pdf', 'page': 0, 'score': 0.40924692153930664, 'preview': 'requirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\\nongo...'}, {'source': 'Deep learning.pdf', 'page': 0, 'score': 0.40924692153930664, 'preview': 'requirements, sensitivity to data quality and bias, and difficulties in interpretability and reliability—motivating\\nongo...'}], 'summary': 'Deep learning is a technique that enables computers to make decisions by processing data through multiple layers of artificial neurons. This process transforms raw data into meaningful features, allowing computers to learn and make informed decisions.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what is deep learning\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695e1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
